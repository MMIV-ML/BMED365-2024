
Slide #1

Welcome to this module on Future Perspective in Medical Data Science as part of the DIGI 116 course at UiB. I am Arvid Lundervold, being a Professor emeritus in Medical Information Technology and Physiology at The Department of Biomedicine.

Next slide - Slide #2


Slide #2

I will guide you through a collection of diverse topics within medical data science and AI - both giving motivation and some insight into the future of medicine.

First, we will be motivated by quoting three giants in science: Lord Kelvin, Richard Feynman, and Demis Hassabis.

“When you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind: it may be the beginning of knowledge, but you have scarcely, in your thoughts, advanced the stage of science, whatever the matter may be” - that was Lord Kelvin from 1883.

Then Richard Feynman, the famous physicist:
“People who wish to analyze nature without using mathematics must settle for a reduced understanding”

And finally, the contemporary genius and this year winner of the Albert Lasker Award for Basic Medical Research for the invention of AlphaFold - the artificial intelligence system that solved the long-standing challenge of predicting the three-dimensional structure of proteins from the one-dimensional sequence of their amino acids, Demis Hassabis at Google DeepMind, London:
“AT its most fundamental level, I think biology - and likely medicine and the human body - can be thought of as an information processing system, albeit an extraordinarily complex and dynamic one. Just as mathematics turned out to be the right description language for physics, biology may turn out to be the perfect type of regime for the application of AI”.

In the following we will touch upon the topics of “Computational medicine”; ”Big data” and “Rich data”; Network science and “Patient Similarity Networks”, or PSN; Foundation models (being multimodal artificial neural networks - addressing both text, tabular data, speech, vision, images, gene sequences, and sensor data); and the concepts of “P4” that is: Predictive, Preventive, Personalized, and Participatory medicine and finally  “Precision medicine”.

Next slide - Slide #3


Slide #3

Traditionally, and also in the future, medical practice will be conducted by clinicians. However, there will likely arise a new profession or discipline - “body engineers” - that will deal with the computational subdisciplines (here denoted X) in collaboration and interaction with patients, clinicians, and researchers. Within the field of biomedicine, dry-lab work will be as important as wet-lab, and the concept of “moist-lab” will characterize such transdisciplinary lab teams. 

Next slide - Slide #4


Slide #4

So what this is “Computational medicine”?
We might define it as “the application of theory, methods, and technologies from engineering, mathematics, computational sciences, and more, aiming for a deeper understanding and better treatment of disease and disease processes.”

How is this performed in modern medicine? 
Given the large, rich, and highly heterogeneous sources data we already deal with, and more to be added in the coming years - data generation related to physiological recordings like ECG and EEG, digital pathology, clinical CT, MRI and PET scanners, clinical chemistry lab, sequencing and multi-omics, smartphones and wearable, questionnaires and video-recordings, and much of these connected to electronic health record systems (EHR) where 1000s of Terabytes can be generated across our lifespans - computational medicine is an approach to acquire, organize, analyze, visualize and model these data in order to diagnose, treat and possibly predict diseases and disease processes.

Next slide - Slide #5


Slides #5

Computational medicine deals with different scales, both temporally and spatially, and spans structure and function in molecules and organisms and interactions between organisms of different kinds.

The spatial scale ranges from atoms, proteins (nanometers), cells (micrometers), tissue at the mesoscale, the scale between the microscale and the macroscale, organs, organ systems, the complete organism, and populations of organisms.
Temporal scales range from microseconds (for example ion channel gating), milli-seconds such as diffusion and cell signaling, seconds (motility and muscle action), protein turnover at a scale of 1 million seconds or about 10 days, human lifetime, and even evolutionary time. 
Computational medicine makes use of a plethora of methods: pathway models and network science and graph theory, stochastic models, ordinary differential equations, denoted ODEs (for example compartment modeling based on concservation laws), continuum models, or partial differential equations, denoted PDEs (used in tumor growth models, or in outbreak science). 
The application can range from channelopathy to the brain-gut-microbiota axis in irritable bowel syndrome (IBS).

Next slide - Slide #6


Slide #6

Here is illustrated the use of computational models, more specifically “compartment models” formulated as systems of ODEs or PDEs capturing both the process of Outbreak science - for example, the SIR-model for the COVID-19 pandemic illustrated to the left, and the Renal function - estimation of glomerular filtration rate (GFR), non-invasively from abdominal dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) illustrated to the right.

Being aware of the generic nature of these models, one should have in mind the wisdom expressed by Eugene Wigner, the Hungarian-American theoretical physicist who received the Nobel Prize in physics in 1963, and published 1960 the paper in Communication of Pure and Applied Mathematics entitled: “The unreasonable effectiveness of mathematics in the natural sciences”, saying “... The miracle of the appropriateness of the language of mathematics for the formulation of the laws of physics is a wonderful gift which we neither understand nor deserve.”

Next slide - Slide #7


Slide #7

A large class of models in computational medicine and medical AI can be described with this formalism. We can think of these models as a function or and algorithm taking data X as input and providing y as output. More specifically, X can be tabular as in a spreadsheet with rows (observations) and columns (features, variables), or a relational database, text, images, a molecular sequence, speech, sound, or video; y can be an outcome (diagnosis, prognosis, tumor grade, class, label), or a continuous value or vector of values as in a regression. The model, algorithm, or function f is characterized by parameters, denoted theta, that can be learned from data (trainable parameters) or estimated from data. 
The symbol tilde denotes approximation, hence the model design can be regarded as function approximation or an optimization problem where one wants to minimize the gap between the predicted outcome, often denoted “y-hat” and the desired outcome or ground truth y. In a linear regression problem, y equals “a” times “x” plus “b”, theta is the pair (a,b). In a simple exponential growth model, theta is the growth rate. In a large language model, built as an artificial neural network, the theta can have billions of components, each representing a trainable parameter,  being in the same order as the number of neurons in the brain (100 billion), each having on average 10000 synaptic connections. GPT-4 has more than a trillion parameters. 

Next slide - Slide #8


Slide #8

Illustration of our formalism applied to a predictive medical AI model f, where the input X is highly heterogeneous, and the output y is related to a predictive task such as diagnosis, phenotype, overall survival, etc. In the training phase of such a model, the parameter-vector theta is learned from the data, given the architecture of the network and the loss function (optimization criteria).  This is time-consuming (days) and compute intensive (multiple high-end GPUs, millions of $ for large models). When the trained model is applied to new data (inference), the parameters are fixed, and the compute time can be very short, often seconds or almost instantaneous. 

Next slide - Slide #9


Slide #9

In medical data science and AI, large and heterogenic data are typically collected, often multiple times, which results in longitudinal for each patient or “experimental unit”. 
Proper data organization and well-orchestrated data management are crucial. 

In the interest of high-quality and high-functional hospital infrastructure, or for alignment to open science and reproducible research, the FAIR principle comes into action. 

Data should be Findable (FAIR data is easily discoverable by both humans and machines); Accessible (FAIR data is accessible to both humans and machines); Interoperable (FAIR data is interoperable with other data sets and software tools); Reusable (FAIR data is reusable by both humans and machines).

Next slide - Slide #10


Slide #10

Examples of two major repositories of cancer data supporting Open science and reproducible research are the TCGA (The Cancer Genome Atlas) and TCIA (The Cancer Imaging Archive). The images shown are multiparametric MRI recordings from a patient with glioblastoma. Such data, being pseudonymized and patients having given their informed consent, are often used to train large predictive models in neuro-oncology incorporating standardized protocols for brain tumor imaging.

Next slide - Slide #11


Slide #11

Example of a 4D dataset X, derived from functional magnetic resonance of the brain (fMRI). The data has been segmented into pre-define brain regions (color-coded voxels). The information contained in the “resting state fMRI”  recording is interrelated in both space and time. Applications are basic neurocognitive research and pre-surgical mapping of functionally important, subject-specific brain regions (language area, motor areas, etc.)

Next slide - Slide #12


Slide #12

Multimodal data from the same repositories. The upper left are histological sections from the resected brain tumor. The lower part shows the multiparametric MRI recordings for the same patient. Metadata (demographics, tumor grade, genetic profiling, etc.) is also important information that can used to build and apply models for subject-specific prediction of overall survival.

Next slide - Slide #13


Slide #13

This slide shows multimodal image data from a brain tumor in the upper left. In the upper right, Imaging Mass Cytometry (IMC) is illustrated. IMC is a powerful technique that allows for the simultaneous detection and visualization of up to 40 different protein targets in tissue samples and has had a significant impact on a wide range of research areas, including cancer biology, immunology, and neuroscience.
The lower part of the slide shows how tissue segmentation can work. The input is multiparametric MRI recording and the output is the predicted class or tissue type (fat, muscle, gray matter, white matter, cerebrospinal fluid, or air/bone) for each voxel within the volume of interest. The trained model is a k-nearest neighbor classifier, and the training set is illustrated by the tissue-specific color-coded manual delineations (sampling) in the multiparametric dataset.

Next slide - Slide #14


Slide #14

This slide illustrates how network science and graph theory can be used to represent and compute complex medical data, that is: real-world observation -> abstraction -> computational representation.
On the upper right, we see how a classical problem, the Seven Bridges of Königsberg, could be solved by Leonard Euler in 1737 introducing graph theory and their topology. The problem was to devise a walk through the city that would cross each of those bridges once and only once. Euler showed that no such walk is possible. He did this by constructing a mathematical model of the problem, which is now known as a graph. A graph is a set of points, called vertices, and a set of lines connecting pairs of vertices, called edges. In the graph for the Seven Bridges of Königsberg, each vertex represents a different landmass or island, and each edge represents a bridge. Euler proved that in order for there to be a walk that crosses each edge of a graph once and only once, the number of edges connected to each vertex must be even. However, in the graph for the Seven Bridges of Königsberg, there are three vertices (the two islands and one of the mainland portions) that have an odd number of edges connected to them. Later shown: the graph must be connected and have exactly zero or two nodes of odd degree (an Eulerian path). Therefore, there cannot be a walk that crosses each bridge once and only once. It showed that mathematical models could be used to solve real-world problems, and it laid the foundation for the field of graph theory. 
To the left, we see different networks at different biological levels, and their graph representations related to irritable bowel syndrome (IBS).

Next slide - Slide #15


Slide #15

Patient Similarity Networks (PSNs) are a type of network-based approach to precision medicine. PSNs are constructed by clustering patients based on their similarity in terms of their clinical and/or biomolecular features. Once a PSN is constructed, it can be used to identify similar patients to a new patient, which can be helpful for a variety of tasks, such as:
Diagnosis: PSN can be used to identify similar patients to a new patient with a challenging-to-find or rare diagnosis. This can help clinicians to narrow down the list of possible diagnoses and to develop a more informed treatment plan.  Prognosis: PSN can be used to identify similar patients to a new patient with a known diagnosis. This can help clinicians to predict the patient's prognosis and to develop a more personalized treatment plan.  Treatment selection: PSN can be used to identify similar patients to a new patient who have responded well to a particular treatment. This can help clinicians to select the most effective treatment for the new patient.   Clinical trial matching: PSN can be used to identify similar patients to a new patient who are eligible to participate in clinical trials. This can help patients to access the latest and most promising treatments.

Next slide - Slide #16


Slide #16

Neural networks and graphs are relevant concepts both in medical data science and computational medicine, and also to represent knowledge and measurements in neuroscience and brain research. Neurons in the brain are connected to each other by synapses. These connections form a complex network, which can be modeled using a graph. In a graph, each neuron is represented by a node, and each synapse is represented by an edge. See the formal description of a graph at the bottom of the slide. Graph theory can be used to study the structure and function of brain networks. For example, graph theory can be used to identify the most important nodes in a network, to measure the efficiency of the network, and to identify patterns of connectivity between different regions of the brain. Graph theory has been used to study a wide range of neurological disorders, including Alzheimer's disease, Parkinson's disease, and schizophrenia.  Inter-regional neuronal communication refers to communication between neurons in different regions of the brain. This type of communication is essential for many brain functions, such as perception, cognition, and action, and can be simplified by dividing it into feedforward communication and feedback communication. Many of the same principles have inspired the design of artificial neural networks, from simple feed-forward perceptrons to large language models, with multimodal capabilities (as we find in the brain). 

Next slide - Slide #17


Slide #17

The AI wave is the current period of rapid growth and development in the field of artificial intelligence. This wave is being driven by a number of factors, including the increasing availability of data, the development of more powerful computing hardware, and the development of new AI algorithms. During the last year, we have seen a revolution in generative AI (genAI). This is a type of artificial intelligence that can create new content, such as text, images, music, and code. It is trained on large datasets of existing content (self-supervised learning), and then uses that knowledge to generate new content that is similar to the training data. 
The AI wave is having a major impact on a wide range of industries and sectors, including healthcare, finance, manufacturing, transportation, education, law, and even art. The AI wave is also having a major impact on the way we work and live and it is important to place yourself in this picture, regarding both your mindset, skillsets, and toolsets.

Next slide - Slide #17


Slide #18

Representation learning is a type of machine learning that allows computers to learn from data without being explicitly programmed. Representation learning algorithms work by extracting features from data and then, often hierarchically, using those features to represent the data in a way that is useful for learning tasks and is likely an important component to achieving so-called artificial general intelligence (AGI).  One could ask if the same mechanisms take place in the brain, to achieve human intelligence (HI), using biological and evolutionary principles. Representation learning, generative AI, and biological processes share interesting similarities in how they encode data, learn, and generate new information. In biology, neurons encode stimuli from the environment, much like how representation learning in AI extracts useful features from raw data. Learning mechanisms also show parallels; biological systems use Hebbian learning to strengthen neural connections based on experience, while AI uses backpropagation to adjust network weights. Both biological systems and generative AI models have the ability to create new, complex structures.  They exhibit adaptability and hierarchical complexity, adjusting to new conditions and organizing functions at multiple levels. The parallels between representation learning, generative AI, and biological processes are fascinating and suggest that there may be deep connections between these fields. These parallels not only offer a fascinating avenue for interdisciplinary research but also provide insights that could be mutually beneficial.

Next slide - Slide #19


Slide #19

The concepts of embedding projectors, Word2Vec, deep learning, and fMRI studies in the human brain intersect in fascinating ways, particularly in the realm of representation learning and understanding cognitive processes. 
Embedding projectors (and Word2Vec) create a high-dimensional space where words that are contextually similar are located near each other. This is somewhat analogous to how the human brain organizes semantic information. In the brain, neurons that fire together in response to similar stimuli tend to strengthen their connections, creating a form of semantic mapping.
Deep learning algorithms, especially convolutional neural networks (CNNs) and recurrent neural networks (RNNs), have architectures that are inspired by the hierarchical organization of the human brain. In both systems, lower-level features are processed first (e.g., edges and corners in vision, phonemes in language), and higher-level features (e.g., objects, semantics) are built upon these.
Studies have used fMRI to observe how different words or concepts activate different regions in the brain, showing a form of semantic mapping that could be likened to word embeddings.  More recently, researchers have used large language models (LLMs) to generate stimuli to activate specific parts of the brain, analyzing fMRI data to identify patterns of brain activity associated with different types of language tasks, and using semantics maps to study the relationship between LLMs and the brain.

Next slide - Slide #20


Slide #20

The analogy "Uncrumpling paper balls is what machine learning is about" (a quotefrom Francois Chollet, the inventor of the Keras high-level deep learning library) is a helpful way to understand the concept of representation and transformation in machine learning. When we crumple a piece of paper, we transform it from a flat, 2D object to a 3D object with a complex shape. However, the underlying information about the paper, such as its size, color, and texture, is still preserved. Machine learning algorithms work by learning representations of data. These representations are abstractions of the data that capture the most important information. For example, a machine learning algorithm that is trained to recognize different types of tumors or verbal descriptions of symptoms might learn to represent images of tumors or sequences of words as vectors of numbers. These vectors would capture the most important information about the images, such as the shape and texture of the tumor or the semantics in the symptom description.

Next slide - Slide #21


Slide #21

After the release of ChatGPT-4 by Open AI on March 14, 2023, the New England Journal of Medicine started to publish a series of review articles and special reports on the development and use of AI and machine learning in clinical medicine.
  GPT-4 as an AI chatbot in medicine offers several advantages but also comes with limitations and risks that need to be carefully considered. One of the key benefits is accessibility; GPT-4 can provide immediate responses to medical queries, making healthcare information more accessible to the general public. It also offers consistency in delivering information, reducing the risk of human error. Additionally, the model can sift through large volumes of medical literature to provide evidence-based responses and can assist in multiple languages, breaking down language barriers in healthcare. Automating routine queries can also be cost-effective, reducing the workload on healthcare professionals.
  However, GPT-4 is not without its limitations. It is important to note that it is not a substitute for professional medical advice, diagnosis, or treatment. The model may lack a detailed comprehension of a patient's medical history and current condition, which is crucial for accurate medical advice. Ethical concerns related to patient confidentiality and data security are also present. There is a risk that users may misunderstand or misinterpret the information provided, and the model may not have the most up-to-date medical guidelines or research data. The recent introduction of ChatGP Plugins will, however, make it possible for ChatGPT to consult the internet,  real-time data, and online computing resources such as Mathematica.
  The risks include the potential for misdiagnosis due to incorrect or overly generalized information, and there is a danger that people may overly rely on the chatbot for serious medical issues, delaying proper medical attention. Legal liability and the use of AI in healthcare raises also ethical questions, such as who is responsible for errors or misjudgments.

Next slide - Slide #22


Slide #22

The landscape of artificial intelligence and machine learning is diverse, offering various approaches to problem-solving. Traditional machine learning, for instance, is more specialized and often relies on hand-crafted features and algorithms like decision trees or logistic regression. In contrast, large-scale multimodal AI leverages deep learning to process multiple types of data, such as text, images, and audio, all at once. These models are generally more complex and computationally intensive. When it comes to learning paradigms, supervised learning uses a labeled dataset to train models, where each example is paired with an output label. The model is corrected based on its predictions, making it effective but often data-hungry. Self-supervised learning, on the other hand, uses the data itself for supervision, making it useful when labeled data is scarce or expensive to obtain. In terms of task specialization, single-task AI models excel at one specific task but are not flexible enough to handle others. Multitask models are designed to perform multiple tasks simultaneously, often sharing some layers or representations, which can lead to more robust and generalized models. 
  Adding to this, "parameter tuning" and "fine-tuning and prompting" represent different approaches to model optimization. Parameter tuning involves adjusting the hyperparameters of a model to improve its performance on a specific task. This is common in traditional machine learning where you might tune parameters like learning rate, tree depth, or kernel type. Fine-tuning and prompting are more prevalent in large-scale models like GPT-4. Fine-tuning involves taking a pre-trained model and training it further on a specific dataset or task. Prompting, on the other hand, is about crafting specific input prompts to guide the model's output without additional training, effectively leveraging the model's existing knowledge to perform a task. Prompting a model is both a science and an art, and is coined “prompt engineering”.

Next slide - Slide #23


Slide #23

Foundation models for generalist medical artificial intelligence (GMAI) are a new generation of AI models that are being developed to address the limitations of current medical AI systems. GMAI models are trained on massive datasets of medical data, including text, images, and genomic data. This allows GMAI models to learn complex relationships between different types of medical data, and to perform a wide range of tasks. These include - Diagnosis: GMAI models can be used to diagnose diseases more accurately and efficiently than traditional methods. Treatment planning: GMAI models can be used to develop personalized treatment plans for patients, taking into account their individual medical history and other factors. Drug discovery: GMAI models can be used to identify new drug targets and to develop new drugs more quickly and efficiently. Clinical research: GMAI models can be used to accelerate clinical research by helping researchers identify patterns and trends in medical data that would be difficult or impossible to find manually. GMAI models are still under development, but they have the potential to revolutionize the field of medicine. By enabling doctors to make more accurate and informed decisions helping to improve the quality of care for patients. However, significant challenges in terms of validation, interpretability, and ethics must be addressed to fully realize their potential in healthcare settings.

Next slide - Slide #24


Slide #24

P4 Medicine and Precision Medicine are closely related concepts that aim to revolutionize healthcare by making it more personalized, predictive, and preventive. P4 Medicine stands for Predictive, Preventive, Personalized, and Participatory medicine. 
 It goes beyond the scope of traditional healthcare by not only focusing on treatment but also emphasizing the importance of predicting diseases before they occur, preventing them through proactive measures, and involving patients in their own healthcare decisions. The participatory aspect of P4 Medicine encourages patients to be active contributors to their health, often facilitated by technologies like wearable devices that monitor various health metrics. Precision Medicine, on the other hand, is more focused on tailoring medical treatment to the individual characteristics of each patient. It takes into account factors like genetics, lifestyle, and environment to create highly personalized treatment plans. In essence, it moves away from the "one-size-fits-all" approach that has been prevalent in medicine for years. For example, in the field of oncology, Precision Medicine might involve genetic testing to determine which specific type of chemotherapy would be most effective for a particular patient's form of cancer.

Both P4 and Precision Medicine are heavily reliant on data, from genetic information to lifestyle metrics, and they both utilize advanced technologies like big data analytics, machine learning, and even foundation models in AI to analyze this data and derive actionable insights.

Next slide - Slide #25


Slide #25

The integration of computational medicine and data science is transforming healthcare, offering both challenges and opportunities across various spatio-temporal scales and modalities. On one hand, these technologies enable more precise diagnostics, predictive analytics, and personalized treatment plans. They allow for the analysis of data from the molecular level to whole populations, and from real-time monitoring to long-term studies. This multi-scale, multi-modal approach can provide a more comprehensive understanding of health and disease.
  However, this integration also presents challenges. Data privacy and security and ethical implications are major concerns, especially when dealing with sensitive health information. Additionally, the sheer volume and complexity of medical data require robust computational tools and algorithms, which in turn necessitate a change in mindset, skillset, and toolset for healthcare professionals.
  The mindset change involves embracing a more data-driven approach to medicine, where decisions are made based on comprehensive data analysis rather than solely on clinical judgment. This requires a cultural shift within healthcare institutions to value data as a critical component of patient care.  The skillset change is equally important. Medical professionals need to become proficient in data analytics, machine learning, and even coding to some extent. This does not mean they need to become data scientists, but a basic understanding of these fields is becoming increasingly essential. 
  Finally, the toolset must evolve. Traditional medical tools are being supplemented or even replaced by computational platforms that can analyze large datasets, run complex simulations, and provide real-time analytics. These tools are not just for researchers but are becoming integral parts of the clinical setting.

Next slide - Slide #26


Slide #26

Thanks for joining the journey of present and future perspectives on medical data science, computational medicine, and AI.  This slide illustrates the common misconception that flocking movement is organized and coordinated by a leader bird - what about governing principles of medical data science?

In reality, flocking is governed by local rules and interactions among individual birds, not by a single leader.  Just as each bird in a flock reacts to its immediate neighbors, medical data science is about interconnected systems—ranging from molecular biology to patient care and even to healthcare policy. It is not just about applying a machine learning model to a dataset but involves understanding the context, the ethics, and the implications of that model. Data quality, interpretability, and fairness are key principles that guide the application of data science in medicine. Moreover, just as flocking requires each bird to be aware of its immediate surroundings, medical data science requires a multi-disciplinary approach. It is not solely the domain of data scientists or medical doctors; it requires collaboration across fields, including biology, epidemiology, ethics, and even law, to create solutions that are both technically sound and ethically responsible. So, while it may be tempting to look for a "leader" or a single solution that will solve all problems, the reality is much more complex. The governing principles of medical data science are about ensuring that the technology serves to enhance healthcare in a way that is ethical, equitable, and grounded in sound scientific practice. Just as there is no single bird leading the flock, there is no single algorithm or technology that will solve all the challenges in medical data science. It is a collective effort, guided by a set of core principles and collaborative expertise.

This was the last slide - thanks for listening!


BETTER:
Slide #1.  Welcome to this module on Future Perspective in Medical Data Science as part of the DIGI 116 course at UiB. I am Arvid Lundervold, being a Professor emeritus in Medical Information Technology and Physiology at The Department of Biomedicine. Next slide  please - Slide #2. 

Slide #2.  I will guide you through a collection of diverse topics within medical data science and AI - both giving motivation and some insight into the future of medicine. First, we will be motivated by quoting three giants in science: Lord Kelvin, Richard Feynman, and Demis Hassabis. “When you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind: it may be the beginning of knowledge, but you have scarcely, in your thoughts, advanced the stage of science, whatever the matter may be” - that was Lord Kelvin from 1883. Then Richard Feynman, the famous physicist: “People who wish to analyze nature without using mathematics must settle for a reduced understanding” And finally, the contemporary genius and this year winner of the Albert Lasker Award for Basic Medical Research for the invention of AlphaFold - the artificial intelligence system that solved the long-standing challenge of predicting the three-dimensional structure of proteins from the one-dimensional sequence of their amino acids, Demis Hassabis at Google DeepMind, London: “AT its most fundamental level, I think biology - and likely medicine and the human body - can be thought of as an information processing system, albeit an extraordinarily complex and dynamic one. Just as mathematics turned out to be the right description language for physics, biology may turn out to be the perfect type of regime for the application of AI”. In the following we will touch upon the topics of “Computational medicine”; ”Big data” and “Rich data”; Network science and “Patient Similarity Networks”, or PSN; Foundation models (being multimodal artificial neural networks - addressing both text, tabular data, speech, vision, images, gene sequences, and sensor data); and the concepts of “P4” that is: Predictive, Preventive, Personalized, and Participatory medicine and finally “Precision medicine”. Next slide - Slide #3. 

Slide #3.  Traditionally, and also in the future, medical practice will be conducted by clinicians. However, there will likely arise a new profession or discipline - “body engineers” - that will deal with the computational subdisciplines (here denoted X) in collaboration and interaction with patients, clinicians, and researchers. Within the field of biomedicine, dry-lab work will be as important as wet-lab, and the concept of “moist-lab” will characterize such transdisciplinary lab teams. Next slide - Slide #4.

Slide #4.  So what this is “Computational medicine”? We might define it as “the application of theory, methods, and technologies from engineering, mathematics, computational sciences, and more, aiming for a deeper understanding and better treatment of disease and disease processes.” How is this performed in modern medicine? Given the large, rich, and highly heterogeneous sources data we already deal with, and more to be added in the coming years - data generation related to physiological recordings like ECG and EEG, digital pathology, clinical CT, MRI and PET scanners, clinical chemistry lab, sequencing and multi-omics, smartphones and wearable, questionnaires and video-recordings, and much of these connected to electronic health record systems (EHR) where 1000s of Terabytes can be generated across our lifespans - computational medicine is an approach to acquire, organize, analyze, visualize and model these data in order to diagnose, treat and possibly predict diseases and disease processes. Next slide - Slide #5.

Slides #5.  Computational medicine deals with different scales, both temporally and spatially, and spans structure and function in molecules and organisms and interactions between organisms of different kinds. The spatial scale ranges from atoms, proteins (nanometers), cells (micrometers), tissue at the mesoscale, the scale between the microscale and the macroscale, organs, organ systems, the complete organism, and populations of organisms. Temporal scales range from microseconds, for example ion channel gating, milli-seconds such as diffusion and cell signaling, seconds, motility and muscle action, protein turnover at a scale of 1 million seconds or about 10 days, human lifetime, and even evolutionary time. Computational medicine makes use of a plethora of methods: pathway models and network science and graph theory, stochastic models, ordinary differential equations, denoted ODEs, for example compartment modeling based on concservation laws, continuum models, or partial differential equations, denoted PDEs, used in tumor growth models, or in outbreak science. The application can range from channelopathy to the brain-gut-microbiota axis in irritable bowel syndrome, IBS. Next slide - Slide #6 .

Slide #6. Here is illustrated the use of computational models, more specifically “compartment models” formulated as systems of ODEs or PDEs capturing both the process of Outbreak science - for example, the SIR-model for the COVID-19 pandemic illustrated to the left, and the Renal function - estimation of glomerular filtration rate, GFR, non-invasively from abdominal dynamic contrast-enhanced magnetic resonance imaging, DCE-MRI, illustrated to the right. Being aware of the generic nature of these models, one should have in mind the wisdom expressed by Eugene Wigner, the Hungarian-American theoretical physicist who received the Nobel Prize in physics in 1963, and published 1960 the paper in Communication of Pure and Applied Mathematics entitled: “The unreasonable effectiveness of mathematics in the natural sciences”, saying “... The miracle of the appropriateness of the language of mathematics for the formulation of the laws of physics is a wonderful gift which we neither understand nor deserve.” Next slide - Slide #7. 

Slide #7. A large class of models in computational medicine and medical AI can be described with this formalism. We can think of these models as a function or and algorithm taking data X as input and providing y as output. More specifically, X can be tabular as in a spreadsheet with rows, observations, and columns, features, variables, or a relational database, text, images, a molecular sequence, speech, sound, or video; y can be an outcome, diagnosis, prognosis, tumor grade, class, label, or a continuous value or vector of values as in a regression. The model, algorithm, or function f is characterized by parameters, denoted theta, that can be learned from data, trainable parameters, or estimated from data. The symbol tilde denotes approximation, hence the model design can be regarded as function approximation or an optimization problem where one wants to minimize the gap between the predicted outcome, often denoted “y-hat” and the desired outcome or ground truth y. In a linear regression problem, y equals “a” times “x” plus “b”, theta is the pair (a,b). In a simple exponential growth model, theta is the growth rate. In a large language model, built as an artificial neural network, the theta can have billions of components, each representing a trainable parameter, being in the same order as the number of neurons in the brain , 100 billion, each having on average 10000 synaptic connections. GPT-4 has more than a trillion parameters. Next slide - Slide #8. 

Slide #8. Illustration of our formalism applied to a predictive medical AI model f, where the input X is highly heterogeneous, and the output y is related to a predictive task such as diagnosis, phenotype, overall survival, etc. In the training phase of such a model, the parameter-vector theta is learned from the data, given the architecture of the network and the loss function, optimization criteria. This is time-consuming , days and compute intensive, multiple high-end GPUs spending millions of dollars for large models. When the trained model is applied to new data , inference, the parameters are fixed, and the compute time can be very short, often seconds or almost instantaneous. Next slide - Slide #9. 

Slide #9. In medical data science and AI, large and heterogenic data are typically collected, often multiple times, which results in longitudinal for each patient or “experimental unit”. Proper data organization and well-orchestrated data management are crucial. In the interest of high-quality and high-functional hospital infrastructure, or for alignment to open science and reproducible research, the FAIR principle comes into action. Data should be Findable, FAIR data is easily discoverable by both humans and machines; Accessible, FAIR data is accessible to both humans and machines; Interoperable, FAIR data is interoperable with other data sets and software tools; Reusable, FAIR data is reusable by both humans and machines. Next slide - Slide #10. 

Slide #10. Examples of two major repositories of cancer data supporting Open science and reproducible research are the TCGA, The Cancer Genome Atlas, and TCIA, The Cancer Imaging Archive. The images shown are multiparametric MRI recordings from a patient with glioblastoma. Such data, being pseudonymized and patients having given their informed consent, are often used to train large predictive models in neuro-oncology incorporating standardized protocols for brain tumor imaging. Next slide - Slide #11 .

Slide #11 . Example of a 4D dataset X, derived from functional magnetic resonance of the brain, fMRI. The data has been segmented into pre-define brain regions, color-coded voxels. The information contained in the “resting state fMRI” recording is interrelated in both space and time. Applications are basic neurocognitive research and pre-surgical mapping of functionally important, subject-specific brain regions (language area, motor areas, etc.. Next slide - Slide #12 .

Slide #12. Multimodal data from the same repositories. The upper left are histological sections from the resected brain tumor. The lower part shows the multiparametric MRI recordings for the same patient. Metadata, demographics, tumor grade, genetic profiling, etc.,  is also important information that can used to build and apply models for subject-specific prediction of overall survival. Next slide - Slide #13 .

Slide #13. This slide shows multimodal image data from a brain tumor in the upper left. In the upper right, Imaging Mass Cytometry, IMC, is illustrated. IMC is a powerful technique that allows for the simultaneous detection and visualization of up to 40 different protein targets in tissue samples and has had a significant impact on a wide range of research areas, including cancer biology, immunology, and neuroscience. The lower part of the slide shows how tissue segmentation can work. The input is multiparametric MRI recording and the output is the predicted class or tissue type (fat, muscle, gray matter, white matter, cerebrospinal fluid, or air/bone) for each voxel within the volume of interest. The trained model is a k-nearest neighbor classifier, and the training set is illustrated by the tissue-specific color-coded manual delineations (sampling) in the multiparametric dataset. Next slide - Slide #14.

Slide #14.  This slide illustrates how network science and graph theory can be used to represent and compute complex medical data, that is: real-world observation -> abstraction -> computational representation. On the upper right, we see how a classical problem, the Seven Bridges of Königsberg, could be solved by Leonard Euler in 1737 introducing graph theory and their topology. The problem was to devise a walk through the city that would cross each of those bridges once and only once. Euler showed that no such walk is possible. He did this by constructing a mathematical model of the problem, which is now known as a graph. A graph is a set of points, called vertices, and a set of lines connecting pairs of vertices, called edges. In the graph for the Seven Bridges of Königsberg, each vertex represents a different landmass or island, and each edge represents a bridge. Euler proved that in order for there to be a walk that crosses each edge of a graph once and only once, the number of edges connected to each vertex must be even. However, in the graph for the Seven Bridges of Königsberg, there are three vertices, the two islands and one of the mainland portions, that have an odd number of edges connected to them. Later shown: the graph must be connected and have exactly zero or two nodes of odd degree, an Eulerian path. Therefore, there cannot be a walk that crosses each bridge once and only once. It showed that mathematical models could be used to solve real-world problems, and it laid the foundation for the field of graph theory. To the left, we see different networks at different biological levels and their graph representations related to irritable bowel syndrome, IBS. Next slide - Slide #15 .

Slide #15. Patient Similarity Networks, PSNs, are a type of network-based approach to precision medicine. PSNs are constructed by clustering patients based on their similarity in terms of their clinical and/or biomolecular features. Once a PSN is constructed, it can be used to identify similar patients to a new patient, which can be helpful for a variety of tasks, such as: 
Diagnosis: PSN can be used to identify similar patients to a new patient with a challenging-to-find or rare diagnosis. This can help clinicians to narrow down the list of possible diagnoses and to develop a more informed treatment plan. Prognosis: PSN can be used to identify similar patients to a new patient with a known diagnosis. This can help clinicians to predict the patient's prognosis and to develop a more personalized treatment plan. Treatment selection: PSN can be used to identify similar patients to a new patient who have responded well to a particular treatment. This can help clinicians to select the most effective treatment for the new patient. Clinical trial matching: PSN can be used to identify similar patients to a new patient who are eligible to participate in clinical trials. This can help patients to access the latest and most promising treatments.  Next slide - Slide #16.

Slide #16.  Neural networks and graphs are relevant concepts both in medical data science and computational medicine, and also to represent knowledge and measurements in neuroscience and brain research. Neurons in the brain are connected to each other by synapses. These connections form a complex network, which can be modeled using a graph. In a graph, each neuron is represented by a node, and each synapse is represented by an edge. See the formal description of a graph at the bottom of the slide. Graph theory can be used to study the structure and function of brain networks. For example, graph theory can be used to identify the most important nodes in a network, to measure the efficiency of the network, and to identify patterns of connectivity between different regions of the brain. Graph theory has been used to study a wide range of neurological disorders, including Alzheimer's disease, Parkinson's disease, and schizophrenia. Inter-regional neuronal communication refers to communication between neurons in different regions of the brain. This type of communication is essential for many brain functions, such as perception, cognition, and action, and can be simplified by dividing it into feedforward communication and feedback communication. Many of the same principles have inspired the design of artificial neural networks, from simple feed-forward perceptrons to large language models, with multimodal capabilities, as we find in the brain. Next slide - Slide #17. 

Slide #17. The AI wave is the current period of rapid growth and development in the field of artificial intelligence. This wave is being driven by a number of factors, including the increasing availability of data, the development of more powerful computing hardware, and the development of new AI algorithms. During the last year, we have seen a revolution in generative AI, or genAI. This is a type of artificial intelligence that can create new content, such as text, images, music, and code. It is trained on large datasets of existing content (self-supervised learning), and then uses that knowledge to generate new content that is similar to the training data. 
The AI wave is having a major impact on a wide range of industries and sectors, including healthcare, finance, manufacturing, transportation, education, law, and even art. The AI wave is also having a major impact on the way we work and live and it is important to place yourself in this picture, regarding both your mindset, skillsets, and toolsets.  Next slide - Slide #18.

Slide #18.  Representation learning is a type of machine learning that allows computers to learn from data without being explicitly programmed. Representation learning algorithms work by extracting features from data and then, often hierarchically, using those features to represent the data in a way that is useful for learning tasks and is likely an important component to achieving so-called artificial general intelligence, AGI. One could ask if the same mechanisms take place in the brain, to achieve human intelligence, HI, using biological and evolutionary principles. Representation learning, generative AI, and biological processes share interesting similarities in how they encode data, learn, and generate new information. In biology, neurons encode stimuli from the environment, much like how representation learning in AI extracts useful features from raw data. Learning mechanisms also show parallels; biological systems use Hebbian learning to strengthen neural connections based on experience, while AI uses backpropagation to adjust network weights. Both biological systems and generative AI models have the ability to create new, complex structures. They exhibit adaptability and hierarchical complexity, adjusting to new conditions and organizing functions at multiple levels. The parallels between representation learning, generative AI, and biological processes are fascinating and suggest that there may be deep connections between these fields. These parallels not only offer a fascinating avenue for interdisciplinary research but also provide insights that could be mutually beneficial. Next slide - Slide #19.

Slide #19. The concepts of embedding projectors, Word2Vec, deep learning, and fMRI studies in the human brain intersect in fascinating ways, particularly in the realm of representation learning and understanding cognitive processes. Embedding projectors, and Word2Vec, create a high-dimensional space where words that are contextually similar are located near each other. This is somewhat analogous to how the human brain organizes semantic information. In the brain, neurons that fire together in response to similar stimuli tend to strengthen their connections, creating a form of semantic mapping. Deep learning algorithms, especially convolutional neural networks, CNNs, and recurrent neural networks, RNNs,  have architectures that are inspired by the hierarchical organization of the human brain. In both systems, lower-level features are processed first, for example edges and corners in vision, and phonemes in language,  and higher-level features, such as objects, or semantics, are built upon these. Studies have used fMRI to observe how different words or concepts activate different regions in the brain, showing a form of semantic mapping that could be likened to word embeddings. More recently, researchers have used large language models, LLMs, to generate stimuli to activate specific parts of the brain, analyzing fMRI data to identify patterns of brain activity associated with different types of language tasks, and using semantics maps to study the relationship between LLMs and the brain. Next slide - Slide #20. 

Slide #20. The analogy "Uncrumpling paper balls is what machine learning is about", a quote from Francois Chollet, the inventor of the Keras high-level deep learning library, is a helpful way to understand the concept of representation and transformation in machine learning. When we crumple a piece of paper, we transform it from a flat, 2D object to a 3D object with a complex shape. However, the underlying information about the paper, such as its size, color, and texture, is still preserved. Machine learning algorithms work by learning representations of data. These representations are abstractions of the data that capture the most important information. For example, a machine learning algorithm that is trained to recognize different types of tumors or verbal descriptions of symptoms might learn to represent images of tumors or sequences of words as vectors of numbers. These vectors would capture the most important information about the images, such as the shape and texture of the tumor or the semantics in the symptom description.  Next slide - Slide #21.

Slide #21.  After the release of ChatGPT-4 by Open AI on March 14, 2023, the New England Journal of Medicine started to publish a series of review articles and special reports on the development and use of AI and machine learning in clinical medicine. GPT-4 as an AI chatbot in medicine offers several advantages but also comes with limitations and risks that need to be carefully considered. One of the key benefits is accessibility; GPT-4 can provide immediate responses to medical queries, making healthcare information more accessible to the general public. It also offers consistency in delivering information, reducing the risk of human error. Additionally, the model can sift through large volumes of medical literature to provide evidence-based responses and can assist in multiple languages, breaking down language barriers in healthcare. Automating routine queries can also be cost-effective, reducing the workload on healthcare professionals. However, GPT-4 is not without its limitations. It is important to note that it is not a substitute for professional medical advice, diagnosis, or treatment. The model may lack a detailed comprehension of a patient's medical history and current condition, which is crucial for accurate medical advice. Ethical concerns related to patient confidentiality and data security are also present. There is a risk that users may misunderstand or misinterpret the information provided, and the model may not have the most up-to-date medical guidelines or research data. The recent introduction of ChatGP Plugins will, however, make it possible for ChatGPT to consult the internet, real-time data, and online computing resources such as Mathematica. The risks include the potential for misdiagnosis due to incorrect or overly generalized information, and there is a danger that people may overly rely on the chatbot for serious medical issues, delaying proper medical attention. Legal liability and the use of AI in healthcare raises also ethical questions, such as who is responsible for errors or misjudgments. Next slide - Slide #22. 

Slide #22.  The landscape of artificial intelligence and machine learning is diverse, offering various approaches to problem-solving. Traditional machine learning, for instance, is more specialized and often relies on hand-crafted features and algorithms like decision trees or logistic regression. In contrast, large-scale multimodal AI leverages deep learning to process multiple types of data, such as text, images, and audio, all at once. These models are generally more complex and computationally intensive. When it comes to learning paradigms, supervised learning uses a labeled dataset to train models, where each example is paired with an output label. The model is corrected based on its predictions, making it effective but often data-hungry. Self-supervised learning, on the other hand, uses the data itself for supervision, making it useful when labeled data is scarce or expensive to obtain. In terms of task specialization, single-task AI models excel at one specific task but are not flexible enough to handle others. Multitask models are designed to perform multiple tasks simultaneously, often sharing some layers or representations, which can lead to more robust and generalized models. Adding to this, "parameter tuning" and "fine-tuning and prompting" represent different approaches to model optimization. Parameter tuning involves adjusting the hyperparameters of a model to improve its performance on a specific task. This is common in traditional machine learning where you might tune parameters like learning rate, tree depth, or kernel type. Fine-tuning and prompting are more prevalent in large-scale models like GPT-4. Fine-tuning involves taking a pre-trained model and training it further on a specific dataset or task. Prompting, on the other hand, is about crafting specific input prompts to guide the model's output without additional training, effectively leveraging the model's existing knowledge to perform a task. Prompting a model is both a science and an art, and is coined “prompt engineering”. Next slide - Slide #2..

Slide #23. Foundation models for generalist medical artificial intelligence, GMAI, are a new generation of AI models that are being developed to address the limitations of current medical AI systems. GMAI models are trained on massive datasets of medical data, including text, images, and genomic data. This allows GMAI models to learn complex relationships between different types of medical data, and to perform a wide range of tasks. These include - Diagnosis: GMAI models can be used to diagnose diseases more accurately and efficiently than traditional methods. Treatment planning: GMAI models can be used to develop personalized treatment plans for patients, taking into account their individual medical history and other factors. Drug discovery: GMAI models can be used to identify new drug targets and to develop new drugs more quickly and efficiently. Clinical research: GMAI models can be used to accelerate clinical research by helping researchers identify patterns and trends in medical data that would be difficult or impossible to find manually. GMAI models are still under development, but they have the potential to revolutionize the field of medicine. By enabling doctors to make more accurate and informed decisions helping to improve the quality of care for patients. However, significant challenges in terms of validation, interpretability, and ethics must be addressed to fully realize their potential in healthcare settings. Next slide - Slide #24. 

Slide #24. P4 Medicine and Precision Medicine are closely related concepts that aim to revolutionize healthcare by making it more personalized, predictive, and preventive. P4 Medicine stands for Predictive, Preventive, Personalized, and Participatory medicine. It goes beyond the scope of traditional healthcare by not only focusing on treatment but also emphasizing the importance of predicting diseases before they occur, preventing them through proactive measures, and involving patients in their own healthcare decisions. The participatory aspect of P4 Medicine encourages patients to be active contributors to their health, often facilitated by technologies like wearable devices that monitor various health metrics. Precision Medicine, on the other hand, is more focused on tailoring medical treatment to the individual characteristics of each patient. It takes into account factors like genetics, lifestyle, and environment to create highly personalized treatment plans. In essence, it moves away from the "one-size-fits-all" approach that has been prevalent in medicine for years. For example, in the field of oncology, Precision Medicine might involve genetic testing to determine which specific type of chemotherapy would be most effective for a particular patient's form of cancer. Both P4 and Precision Medicine are heavily reliant on data, from genetic information to lifestyle metrics, and they both utilize advanced technologies like big data analytics, machine learning, and even foundation models in AI to analyze this data and derive actionable insights. Next slide - Slide #25.

Slide #25. The integration of computational medicine and data science is transforming healthcare, offering both challenges and opportunities across various spatio-temporal scales and modalities. On one hand, these technologies enable more precise diagnostics, predictive analytics, and personalized treatment plans. They allow for the analysis of data from the molecular level to whole populations, and from real-time monitoring to long-term studies. This multi-scale, multi-modal approach can provide a more comprehensive understanding of health and disease. However, this integration also presents challenges. Data privacy and security and ethical implications are major concerns, especially when dealing with sensitive health information. Additionally, the sheer volume and complexity of medical data require robust computational tools and algorithms, which in turn necessitate a change in mindset, skillset, and toolset for healthcare professionals. The mindset change involves embracing a more data-driven approach to medicine, where decisions are made based on comprehensive data analysis rather than solely on clinical judgment. This requires a cultural shift within healthcare institutions to value data as a critical component of patient care. The skillset change is equally important. Medical professionals need to become proficient in data analytics, machine learning, and even coding to some extent. This does not mean they need to become data scientists, but a basic understanding of these fields is becoming increasingly essential. Finally, the toolset must evolve. Traditional medical tools are being supplemented or even replaced by computational platforms that can analyze large datasets, run complex simulations, and provide real-time analytics. These tools are not just for researchers but are becoming integral parts of the clinical setting. Next slide - Slide #26. 

Slide #26. Thanks for joining the journey of present and future perspectives on medical data science, computational medicine, and AI. This slide illustrates the common misconception that flocking movement is organized and coordinated by a leader bird -what about governing principles of medical data science? In reality, flocking is governed by local rules and interactions among individual birds, not by a single leader. Just as each bird in a flock reacts to its immediate neighbors, medical data science is about interconnected systems—ranging from molecular biology to patient care and even to healthcare policy. It is not just about applying a machine learning model to a dataset but involves understanding the context, the ethics, and the implications of that model. Data quality, interpretability, and fairness are key principles that guide the application of data science in medicine. Moreover, just as flocking requires each bird to be aware of its immediate surroundings, medical data science requires a multi-disciplinary approach. It is not solely the domain of data scientists or medical doctors; it requires collaboration across fields, including biology, epidemiology, ethics, and even law, to create solutions that are both technically sound and ethically responsible. So, while it may be tempting to look for a "leader" or a single solution that will solve all problems, the reality is much more complex. The governing principles of medical data science are about ensuring that the technology serves to enhance healthcare in a way that is ethical, equitable, and grounded in sound scientific practice. Just as there is no single bird leading the flock, there is no single algorithm or technology that will solve all the challenges in medical data science. It is a collective effort, guided by a set of core principles and collaborative expertise. This was the last slide - thanks for listening! 


32216 charcters.